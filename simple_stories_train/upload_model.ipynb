{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "from simple_stories_train.models.llama import Llama, LlamaConfig\n",
    "from simple_stories_train.models.model_configs import MODEL_CONFIGS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = wandb.restore(\n",
    "    \"model_step_4824.pt\", run_path=\"dbra/simple-stories/runs/w66ikhs3\", replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a wrapper class for hf that inherits from PyTorchModelHubMixin\n",
    "class LlamaTransformer(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin,\n",
    "    repo_url=\"https://github.com/danbraunai/simple_stories_train\",\n",
    "    language=[\"en\"],\n",
    "    pipeline_tag=\"text-generation\",\n",
    "):\n",
    "    def __init__(self, **config: Any):\n",
    "        super().__init__()\n",
    "        self.llama = Llama(LlamaConfig(**config))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.llama(x)\n",
    "\n",
    "\n",
    "# create model\n",
    "config = MODEL_CONFIGS_DICT[\"d12\"]\n",
    "model = LlamaTransformer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the model weights obtained from wandb\n",
    "state_dict = torch.load(weights.name, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Strip `_orig_mod.` from keys, this appears to be an artifact of wandb\n",
    "new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "model.llama.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...timid girl into a brave explorer. a small puppet hung on a wall , its\n",
      "          -> bright\n",
      "           \n",
      "...over by the fence , a boy named samuel watched kids play baseball. he wanted\n",
      "          -> to\n",
      "           \n",
      ".... \" are you sure you want to follow that ? \" it chattered. \" many have\n",
      "          -> tried\n",
      "           \n",
      "...my porch. i watched the clouds drift by , feeling alone. my old friend samuel\n",
      "          -> had\n",
      "           \n",
      "...was a boy named jose , who often felt alone. one day , as he wandered , a bright\n",
      "          -> light\n",
      "           \n",
      "..., a girl made a wish for a true companion. little did she know , a wise\n",
      "          -> old\n",
      "           \n",
      "...and see what it could do. as anne polished the lantern , a small light appeared.\n",
      "          -> it\n",
      "           \n",
      "...to meet other kids. the camp was amazing ! they had rockets , space suits\n",
      "          -> ,\n",
      "           \n",
      ".... it reminded her of her own happiness. together , they watched the lights flicker and glow ,\n",
      "          -> and\n",
      "           \n",
      "...##aptor , sat on a rock , looking at her reflection. today was special ;\n",
      "          -> she\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "# We perform a sanity check to see if the model is working\n",
    "\n",
    "from simple_stories_train.dataloaders import DatasetConfig, create_data_loader\n",
    "\n",
    "config = DatasetConfig(\n",
    "    tokenizer_file_path=\"tokenizer/stories-3072.json\",\n",
    "    column_name=\"story\",\n",
    "    is_tokenized=False,\n",
    ")\n",
    "loader, tokenizer = create_data_loader(dataset_config=config, batch_size=1, buffer_size=1000)\n",
    "\n",
    "loader = iter(loader)\n",
    "\n",
    "for _ in range(10):\n",
    "    input = next(loader)[\"input_ids\"].to(torch.int)\n",
    "    out = model(input)\n",
    "    assert out[0].shape == torch.Size([1, 1, 50257])\n",
    "\n",
    "    print(\n",
    "        f\"\"\"...{tokenizer.decode(input.tolist()[0][-20:])}\n",
    "          -> {tokenizer.decode(out[0].argmax(-1).tolist()[0][-10:])}\n",
    "           \"\"\".replace(\" ##\", \"\").replace(\" .\", \".\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUB_REPO_NAME = \"lennart-finke/SimpleStories-125M\"\n",
    "model.save_pretrained(HUB_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we upload the model to the hub\n",
    "HUB_REPO_NAME = \"lennart-finke/SimpleStories-125M\"\n",
    "model.push_to_hub(HUB_REPO_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is available on the hub\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "from simple_stories_train.models.model_configs import MODEL_CONFIGS_DICT\n",
    "\n",
    "config = MODEL_CONFIGS_DICT[\"d12\"]\n",
    "model = LlamaTransformer(**config)\n",
    "HUB_REPO_NAME = \"lennart-finke/SimpleStories-125M\"\n",
    "\n",
    "model = model.from_pretrained(HUB_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model output\n",
    "for _ in range(10):\n",
    "    input = next(loader)[\"input_ids\"].to(torch.int)\n",
    "    out = model(input)\n",
    "    print(out[0].argmax(-1).tolist()[0])\n",
    "    assert out[0].shape == torch.Size([1, 1, 50257])\n",
    "\n",
    "    print(\n",
    "        f\"\"\"...{tokenizer.decode(input.tolist()[0][-20:])}\n",
    "          -> {tokenizer.decode(out[0].argmax(-1).tolist()[0][-10:])}\n",
    "           \"\"\".replace(\" ##\", \"\").replace(\" .\", \".\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
